{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best model\n",
    "\n",
    "After we have processed our dataset, we need to find a model to predict new values. We will explore different models to see which ones perform better in this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask import compute\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from dask_ml.preprocessing import Categorizer, StandardScaler, DummyEncoder\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "from dask_ml.model_selection import train_test_split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='4GB')\n",
    "ddf = dd.read_parquet('/home/diego/Coding/code-challenge-2020/data_root/processed/train.parquet', engine='pyarrow')\n",
    "X, y = ddf.drop(['points'], axis=1), ddf['points']\n",
    "# X, y = compute(ddf.drop(['points'], axis=1), ddf[['points']]) # using pandas dataframes only\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')  # Allows us to visualize pipeline\n",
    "num_proc = make_pipeline(StandardScaler())\n",
    "cat_proc = make_pipeline(Categorizer(), DummyEncoder())\n",
    "cat_cols = X.columns.to_list()\n",
    "cat_cols.remove('price')\n",
    "preprocessor = make_column_transformer((num_proc, ['price']),\n",
    "                                       (cat_proc, cat_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Linear Regression\n",
    "\n",
    "First we will create a baseline using a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test set to test scores\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0, shuffle=True)\n",
    "\n",
    "linear_pipeline = make_pipeline(preprocessor, LinearRegression())\n",
    "linear_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipeline.fit(X_train, y_train)\n",
    "print(f\"R2 score is : {r2_score(y_test, linear_pipeline.predict(X_test))}\")\n",
    "print(f\"MSE is : {mean_squared_error(y_test, linear_pipeline.predict(X_test))}\")\n",
    "print(f\"MAE score is : {mean_absolute_error(y_test, linear_pipeline.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipeline = make_pipeline(preprocessor, Ridge(alpha=0.001))\n",
    "ridge_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'ridge__alpha': np.logspace(0, 1, 10),\n",
    "}\n",
    "\n",
    "#metrics = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "#grid_search = GridSearchCV(ridge_pipeline, parameters, scoring=metrics, refit='r2')\n",
    "grid_search = GridSearchCV(ridge_pipeline, parameters) # grid search with 5-fold cv\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R2 score is : {r2_score(y_test, grid_search.predict(X_test))}\")\n",
    "print(f\"MSE is : {mean_squared_error(y_test, grid_search.predict(X_test))}\")\n",
    "print(f\"MAE score is : {mean_absolute_error(y_test, grid_search.predict(X_test))}\")\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_pipeline = make_pipeline(preprocessor, SVR())\n",
    "svr_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_pipeline = make_pipeline(preprocessor, SVR(kernel='rbf'))\n",
    "svr_pipeline.fit(X_train, y_train)\n",
    "print(f\"R2 score is : {r2_score(y_test, svr_pipeline.predict(X_test))}\")\n",
    "print(f\"MSE is : {mean_squared_error(y_test, svr_pipeline.predict(X_test))}\")\n",
    "print(f\"MAE score is : {mean_absolute_error(y_test, svr_pipeline.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'svr__kernel': ['rbf'], 'svr__gamma': [1e-3, 1e-4], 'svr__C': [1, 10, 100, 1000]},\n",
    "]\n",
    "# metrics = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "# grid_search = GridSearchCV(svr_pipeline, param_grid, scoring=metrics, refit='r2')\n",
    "grid_search = GridSearchCV(svr_pipeline, param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"R2 score is : {r2_score(y_test, grid_search.predict(X_test))}\")\n",
    "print(f\"MSE is : {mean_squared_error(y_test, grid_search.predict(X_test))}\")\n",
    "print(f\"MAE score is : {mean_absolute_error(y_test, grid_search.predict(X_test))}\")\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6  ('venv': venv)",
   "name": "python386jvsc74a57bd0b73498e9f68ae58dde2d0ad7bf906ec6ce08a80affbfa7e297dc61534bdf7d7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}