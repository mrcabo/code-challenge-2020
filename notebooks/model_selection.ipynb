{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best model\n",
    "\n",
    "After we have processed our dataset, we need to find a model to predict new values. We will explore different models to see which ones perform better in this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask import compute\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer, TransformedTargetRegressor\n",
    "# from dask_ml.preprocessing import Categorizer, StandardScaler, DummyEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "# from dask_ml.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "# from dask_ml.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# client = Client(n_workers=2, threads_per_worker=2, memory_limit='4GB')\n",
    "ddf = dd.read_parquet('/home/diego/Coding/code-challenge-2020/data_root/processed/train.parquet', engine='pyarrow')\n",
    "# X, y = ddf.drop(['points'], axis=1), ddf['points']\n",
    "X, y = compute(ddf.drop(['points'], axis=1), ddf[['points']]) # using pandas dataframes only. Dask gives more issues...\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')  # Allows us to visualize pipeline\n",
    "num_proc = make_pipeline(StandardScaler())\n",
    "cat_proc = make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n",
    "numerical_columns = ['price']\n",
    "categorical_columns = X.columns.drop('price').to_list()\n",
    "preprocessor = make_column_transformer((num_proc, numerical_columns),\n",
    "                                       (cat_proc, categorical_columns))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=0, shuffle=True)\n",
    "score_matrix = pd.DataFrame(columns=['model', 'R2', 'MSE', 'MAE']).set_index('model')\n",
    "\n",
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Linear Regression\n",
    "\n",
    "First we will create a baseline using a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipeline = make_pipeline(preprocessor, LinearRegression())\n",
    "linear_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipeline.fit(X_train, y_train)\n",
    "d = {\n",
    "    'R2': [r2_score(y_test, linear_pipeline.predict(X_test))],\n",
    "    'MSE': [mean_squared_error(y_test, linear_pipeline.predict(X_test))],\n",
    "    'MAE': [mean_absolute_error(y_test, linear_pipeline.predict(X_test))]\n",
    "}\n",
    "\n",
    "score_matrix = pd.concat([score_matrix, pd.DataFrame(d, index=['linear'])])\n",
    "score_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using 5-fold cross validation to find best parameters\n",
    "ridge_pipeline = make_pipeline(preprocessor, RidgeCV(alphas=np.logspace(-2, 1, 200), cv=5)) \n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "d = {\n",
    "    'R2': [r2_score(y_test, ridge_pipeline.predict(X_test))],\n",
    "    'MSE': [mean_squared_error(y_test, ridge_pipeline.predict(X_test))],\n",
    "    'MAE': [mean_absolute_error(y_test, ridge_pipeline.predict(X_test))]\n",
    "}\n",
    "print(f\"Best alpha: {ridge_pipeline.named_steps['ridgecv'].alpha_}\")\n",
    "score_matrix = pd.concat([score_matrix, pd.DataFrame(d, index=['ridge'])])\n",
    "score_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with some regularization (Ridge regression), the model starts to get better. Let's take a closer look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (ridge_pipeline.named_steps['columntransformer']\n",
    "                 .named_transformers_['pipeline-2'].named_steps['onehotencoder']\n",
    "                 .get_feature_names(input_features=categorical_columns))\n",
    "\n",
    "feature_names = np.concatenate([numerical_columns, feature_names])\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    ridge_pipeline.named_steps['ridgecv'].coef_.transpose(),\n",
    "    columns=['Coefficients'], index=feature_names\n",
    ")\n",
    "ordered_coefs = coefs.loc[coefs.abs().sort_values(by='Coefficients', ascending=False).index]\n",
    "price_idx = ordered_coefs.reset_index()\n",
    "price_idx = price_idx.loc[price_idx['index']=='price']\n",
    "print(f\"Price is in the position {price_idx.index[0]} out of {len(coefs)}\")\n",
    "ordered_coefs = ordered_coefs.iloc[0:30]\n",
    "ordered_coefs.plot(kind='barh', figsize=(9, 7))\n",
    "plt.title('Ridge model')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.subplots_adjust(left=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the 30 bigger coefficients of the model. They tend to be associated with the features 'winery' or 'region_1'. We can further inspect the model using [permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html). This gives us an intuition of which features are most relevant when making predictions. This data confirms our intuition that features like price, region or even the wine's reviewer are more relevant than the variety of the grape for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "r = permutation_importance(ridge_pipeline, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        print(f\"{X.columns[i]:<14}\"\n",
    "              f\"{r.importances_mean[i]:.3f}\"\n",
    "              f\" ± {r.importances_std[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the predicted values againts the real targets. A perfect predictor should produce points that fall along the red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.scatter(y_test, ridge_pipeline.predict(X_test))\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "# plt.text(3, 20, string_score)\n",
    "plt.title('Ridge model')\n",
    "plt.ylabel('Model predictions')\n",
    "plt.xlabel('Truths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svr_pipeline = make_pipeline(preprocessor, SVR(kernel='rbf', C=10, gamma=0.0167)) # Best regressor\n",
    "# svr_pipeline.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# d = {\n",
    "#     'R2': [r2_score(y_test, svr_pipeline.predict(X_test))],\n",
    "#     'MSE': [mean_squared_error(y_test, svr_pipeline.predict(X_test))],\n",
    "#     'MAE': [mean_absolute_error(y_test, svr_pipeline.predict(X_test))]\n",
    "# }\n",
    "\n",
    "# score_matrix = pd.concat([score_matrix, pd.DataFrame(d, index=['svr_rbf'])])\n",
    "# score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_pipeline = make_pipeline(preprocessor, SVR())\n",
    "param_grid = [\n",
    "    {\n",
    "        'svr__kernel': ['rbf'],\n",
    "        'svr__gamma': np.logspace(-3, 1, 10), \n",
    "        'svr__C': [1, 10, 100]\n",
    "    },\n",
    "]\n",
    "grid_search = GridSearchCV(svr_pipeline, param_grid)\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "d = {\n",
    "    'R2': [r2_score(y_test, grid_search.predict(X_test))],\n",
    "    'MSE': [mean_squared_error(y_test, grid_search.predict(X_test))],\n",
    "    'MAE': [mean_absolute_error(y_test, grid_search.predict(X_test))]\n",
    "}\n",
    "print(f\"Best parameter set is: {grid_search.best_params_}\")\n",
    "score_matrix = pd.concat([score_matrix, pd.DataFrame(d, index=['svr_rbf'])])\n",
    "score_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "r = permutation_importance(grid_search, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        print(f\"{X.columns[i]:<14}\"\n",
    "              f\"{r.importances_mean[i]:.3f}\"\n",
    "              f\" ± {r.importances_std[i]:.3f}\")\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.scatter(y_test, ridge_pipeline.predict(X_test))\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "# plt.text(3, 20, string_score)\n",
    "plt.title('Ridge model')\n",
    "plt.ylabel('Model predictions')\n",
    "plt.xlabel('Truths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = make_pipeline(preprocessor, KNeighborsRegressor())\n",
    "param_grid = [\n",
    "    {\n",
    "        'kneighborsregressor__weights': ['uniform','distance'],\n",
    "        'kneighborsregressor__n_neighbors': np.arange(1,20,1),\n",
    "        'kneighborsregressor__p': [1, 2]\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(knn_pipeline, param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "d = {\n",
    "    'R2': [r2_score(y_test, grid_search.predict(X_test))],\n",
    "    'MSE': [mean_squared_error(y_test, grid_search.predict(X_test))],\n",
    "    'MAE': [mean_absolute_error(y_test, grid_search.predict(X_test))]\n",
    "}\n",
    "print(f\"Best parameter set is: {grid_search.best_params_}\")\n",
    "score_matrix = pd.concat([score_matrix, pd.DataFrame(d, index=['knn'])])\n",
    "score_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "If we take a look at the score matrix, we see that the Support vector machine with radial kernel outperforms the rest in all three metrics. This could imply that the data follows a distribution that can't be described using linear operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge-venv",
   "language": "python",
   "name": "challenge-venv"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}